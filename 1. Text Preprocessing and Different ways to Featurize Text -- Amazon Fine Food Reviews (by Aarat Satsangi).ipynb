{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X-cp_kAxCAY2"
   },
   "source": [
    "# <center><b><u>Text Preprocessing and Various Text Featurization Techniques</u></b> <br><br><u><b> Amazon Fine Food Reviews</b></u>\n",
    "<b><div align = 'right'><i> By: <font color = 'darkred'>Aarat Satsangi</font></i></div></b>\n",
    "<div align = 'right'>Referenced from: <a href = 'https://github.com/kushagra414/Amazon-Fine-Food-Reviews-Analysis/blob/master/1.%20Amazon%20Fine%20Food%20Review%20TSNE/Amazon%20Fine%20Food%20Reviews%20Analysis_TSNE.ipynb'>Kushagra Shekhawat</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Information About the Dataset</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X-cp_kAxCAY2"
   },
   "source": [
    "Data Source: https://www.kaggle.com/snap/amazon-fine-food-reviews <br>\n",
    "\n",
    "EDA: https://nycdatascience.com/blog/student-works/amazon-fine-foods-visualization/\n",
    "\n",
    "\n",
    "The Amazon Fine Food Reviews dataset consists of reviews of fine foods from Amazon.<br>\n",
    "\n",
    "Number of reviews: 568,454<br>\n",
    "Number of users: 256,059<br>\n",
    "Number of products: 74,258<br>\n",
    "Timespan: Oct 1999 - Oct 2012<br>\n",
    "Number of Attributes/Columns in data: 10 \n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "1. Id\n",
    "2. ProductId - unique identifier for the product\n",
    "3. UserId - unqiue identifier for the user\n",
    "4. ProfileName\n",
    "5. HelpfulnessNumerator - number of users who found the review helpful\n",
    "6. HelpfulnessDenominator - number of users who indicated whether they found the review helpful or not\n",
    "7. Score - rating between 1 and 5\n",
    "8. Time - timestamp for the review\n",
    "9. Summary - brief summary of the review\n",
    "10. Text - text of the review\n",
    "\n",
    "\n",
    "#### Objective:\n",
    "Given a review, determine whether the review is positive (Rating of 4 or 5) or negative (rating of 1 or 2).\n",
    "\n",
    "<br>\n",
    "[Q] How to determine if a review is positive or negative?<br>\n",
    "<br> \n",
    "[Ans] We could use the Score/Rating. A rating of 4 or 5 could be cosnidered a positive review. A review of 1 or 2 could be considered negative. A review of 3 is nuetral and ignored. This is an approximate and proxy way of determining the polarity (positivity/negativity) of a review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WHC_UQTuCAY4"
   },
   "source": [
    "## <b>Loading the Data</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WHC_UQTuCAY4"
   },
   "source": [
    "The dataset is available in two forms\n",
    "1. .csv file\n",
    "2. SQLite Database\n",
    "\n",
    "In order to load the data, We have used the SQLITE dataset as it easier to query the data and visualise the data efficiently.\n",
    "<br> \n",
    "\n",
    "Here as we only want to get the global sentiment of the recommendations (positive or negative), we will purposefully ignore all Scores equal to 3. If the score id above 3, then the recommendation wil be set to \"positive\". Otherwise, it will be set to \"negative\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <b>Importing All Required Packages </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "import re\n",
    "# Tutorial about Python regular expressions: https://pymotw.com/2/re/\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <b>Reading the Data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect(\"D:/Machine Learning/17.1 - Dataset overview Amazon Fine Food reviews(EDA)/DATA SET/database.sqlite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to load only the filtered data which has score above or below 3. So that we can categorize positive and negative sentiment correctly.\n",
    "\n",
    "i.e positive > 3 > negative "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525809</th>\n",
       "      <td>568450</td>\n",
       "      <td>B001EO7N10</td>\n",
       "      <td>A28KG5XORO54AY</td>\n",
       "      <td>Lettie D. Carter</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1299628800</td>\n",
       "      <td>Will not do without</td>\n",
       "      <td>Great for sesame chicken..this is a good if no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525810</th>\n",
       "      <td>568451</td>\n",
       "      <td>B003S1WTCU</td>\n",
       "      <td>A3I8AFVPEE8KI5</td>\n",
       "      <td>R. Sawyer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1331251200</td>\n",
       "      <td>disappointed</td>\n",
       "      <td>I'm disappointed with the flavor. The chocolat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525811</th>\n",
       "      <td>568452</td>\n",
       "      <td>B004I613EE</td>\n",
       "      <td>A121AA1GQV751Z</td>\n",
       "      <td>pksd \"pk_007\"</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1329782400</td>\n",
       "      <td>Perfect for our maltipoo</td>\n",
       "      <td>These stars are small, so you can give 10-15 o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525812</th>\n",
       "      <td>568453</td>\n",
       "      <td>B004I613EE</td>\n",
       "      <td>A3IBEVCTXKNOH</td>\n",
       "      <td>Kathy A. Welch \"katwel\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1331596800</td>\n",
       "      <td>Favorite Training and reward treat</td>\n",
       "      <td>These are the BEST treats for training and rew...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525813</th>\n",
       "      <td>568454</td>\n",
       "      <td>B001LR2CU2</td>\n",
       "      <td>A3LGQPJCZVL9UC</td>\n",
       "      <td>srfell17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1338422400</td>\n",
       "      <td>Great Honey</td>\n",
       "      <td>I am very satisfied ,product is as advertised,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>525814 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId                      ProfileName  \\\n",
       "0            1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1            2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2            3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3            4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4            5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "...        ...         ...             ...                              ...   \n",
       "525809  568450  B001EO7N10  A28KG5XORO54AY                 Lettie D. Carter   \n",
       "525810  568451  B003S1WTCU  A3I8AFVPEE8KI5                        R. Sawyer   \n",
       "525811  568452  B004I613EE  A121AA1GQV751Z                    pksd \"pk_007\"   \n",
       "525812  568453  B004I613EE   A3IBEVCTXKNOH          Kathy A. Welch \"katwel\"   \n",
       "525813  568454  B001LR2CU2  A3LGQPJCZVL9UC                         srfell17   \n",
       "\n",
       "        HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                          1                       1      5  1303862400   \n",
       "1                          0                       0      1  1346976000   \n",
       "2                          1                       1      4  1219017600   \n",
       "3                          3                       3      2  1307923200   \n",
       "4                          0                       0      5  1350777600   \n",
       "...                      ...                     ...    ...         ...   \n",
       "525809                     0                       0      5  1299628800   \n",
       "525810                     0                       0      2  1331251200   \n",
       "525811                     2                       2      5  1329782400   \n",
       "525812                     1                       1      5  1331596800   \n",
       "525813                     0                       0      5  1338422400   \n",
       "\n",
       "                                   Summary  \\\n",
       "0                    Good Quality Dog Food   \n",
       "1                        Not as Advertised   \n",
       "2                    \"Delight\" says it all   \n",
       "3                           Cough Medicine   \n",
       "4                              Great taffy   \n",
       "...                                    ...   \n",
       "525809                 Will not do without   \n",
       "525810                        disappointed   \n",
       "525811            Perfect for our maltipoo   \n",
       "525812  Favorite Training and reward treat   \n",
       "525813                         Great Honey   \n",
       "\n",
       "                                                     Text  \n",
       "0       I have bought several of the Vitality canned d...  \n",
       "1       Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2       This is a confection that has been around a fe...  \n",
       "3       If you are looking for the secret ingredient i...  \n",
       "4       Great taffy at a great price.  There was a wid...  \n",
       "...                                                   ...  \n",
       "525809  Great for sesame chicken..this is a good if no...  \n",
       "525810  I'm disappointed with the flavor. The chocolat...  \n",
       "525811  These stars are small, so you can give 10-15 o...  \n",
       "525812  These are the BEST treats for training and rew...  \n",
       "525813  I am very satisfied ,product is as advertised,...  \n",
       "\n",
       "[525814 rows x 10 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_sql_query(\"\"\"\n",
    "SELECT *\n",
    "FROM Reviews\n",
    "WHERE score != 3\n",
    "\"\"\",con)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to convert the rating score into sentiment \n",
    "\n",
    "i.e. \n",
    "\n",
    "*positive = 1 (>3)* ,\n",
    "*negative = 0 (<3)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b><font color = 'darkblue'> Converting score into positive (=1) or negative (=0)</font></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_neg (x):\n",
    "    if x<3:\n",
    "        return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, apply the function and convert the score!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525809</th>\n",
       "      <td>568450</td>\n",
       "      <td>B001EO7N10</td>\n",
       "      <td>A28KG5XORO54AY</td>\n",
       "      <td>Lettie D. Carter</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1299628800</td>\n",
       "      <td>Will not do without</td>\n",
       "      <td>Great for sesame chicken..this is a good if no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525810</th>\n",
       "      <td>568451</td>\n",
       "      <td>B003S1WTCU</td>\n",
       "      <td>A3I8AFVPEE8KI5</td>\n",
       "      <td>R. Sawyer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1331251200</td>\n",
       "      <td>disappointed</td>\n",
       "      <td>I'm disappointed with the flavor. The chocolat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525811</th>\n",
       "      <td>568452</td>\n",
       "      <td>B004I613EE</td>\n",
       "      <td>A121AA1GQV751Z</td>\n",
       "      <td>pksd \"pk_007\"</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1329782400</td>\n",
       "      <td>Perfect for our maltipoo</td>\n",
       "      <td>These stars are small, so you can give 10-15 o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525812</th>\n",
       "      <td>568453</td>\n",
       "      <td>B004I613EE</td>\n",
       "      <td>A3IBEVCTXKNOH</td>\n",
       "      <td>Kathy A. Welch \"katwel\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1331596800</td>\n",
       "      <td>Favorite Training and reward treat</td>\n",
       "      <td>These are the BEST treats for training and rew...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525813</th>\n",
       "      <td>568454</td>\n",
       "      <td>B001LR2CU2</td>\n",
       "      <td>A3LGQPJCZVL9UC</td>\n",
       "      <td>srfell17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1338422400</td>\n",
       "      <td>Great Honey</td>\n",
       "      <td>I am very satisfied ,product is as advertised,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>525814 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId                      ProfileName  \\\n",
       "0            1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1            2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2            3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3            4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4            5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "...        ...         ...             ...                              ...   \n",
       "525809  568450  B001EO7N10  A28KG5XORO54AY                 Lettie D. Carter   \n",
       "525810  568451  B003S1WTCU  A3I8AFVPEE8KI5                        R. Sawyer   \n",
       "525811  568452  B004I613EE  A121AA1GQV751Z                    pksd \"pk_007\"   \n",
       "525812  568453  B004I613EE   A3IBEVCTXKNOH          Kathy A. Welch \"katwel\"   \n",
       "525813  568454  B001LR2CU2  A3LGQPJCZVL9UC                         srfell17   \n",
       "\n",
       "        HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                          1                       1      1  1303862400   \n",
       "1                          0                       0      0  1346976000   \n",
       "2                          1                       1      1  1219017600   \n",
       "3                          3                       3      0  1307923200   \n",
       "4                          0                       0      1  1350777600   \n",
       "...                      ...                     ...    ...         ...   \n",
       "525809                     0                       0      1  1299628800   \n",
       "525810                     0                       0      0  1331251200   \n",
       "525811                     2                       2      1  1329782400   \n",
       "525812                     1                       1      1  1331596800   \n",
       "525813                     0                       0      1  1338422400   \n",
       "\n",
       "                                   Summary  \\\n",
       "0                    Good Quality Dog Food   \n",
       "1                        Not as Advertised   \n",
       "2                    \"Delight\" says it all   \n",
       "3                           Cough Medicine   \n",
       "4                              Great taffy   \n",
       "...                                    ...   \n",
       "525809                 Will not do without   \n",
       "525810                        disappointed   \n",
       "525811            Perfect for our maltipoo   \n",
       "525812  Favorite Training and reward treat   \n",
       "525813                         Great Honey   \n",
       "\n",
       "                                                     Text  \n",
       "0       I have bought several of the Vitality canned d...  \n",
       "1       Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2       This is a confection that has been around a fe...  \n",
       "3       If you are looking for the secret ingredient i...  \n",
       "4       Great taffy at a great price.  There was a wid...  \n",
       "...                                                   ...  \n",
       "525809  Great for sesame chicken..this is a good if no...  \n",
       "525810  I'm disappointed with the flavor. The chocolat...  \n",
       "525811  These stars are small, so you can give 10-15 o...  \n",
       "525812  These are the BEST treats for training and rew...  \n",
       "525813  I am very satisfied ,product is as advertised,...  \n",
       "\n",
       "[525814 rows x 10 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data = data.Score.map(pos_neg)\n",
    "data.Score = filtered_data\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, Let us analyse the number of USERS in the whole dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unique = pd.read_sql_query(\"\"\"\n",
    "SELECT Count(*) as reviews , Count(DISTINCT UserId) as Users\n",
    "FROM Reviews\"\"\",con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>Users</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>568454</td>\n",
       "      <td>256059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reviews   Users\n",
       "0   568454  256059"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL NUMBER OF USERS ::  256059\n",
      "TOTAL NUMBER OF REVIEWS ::  568454\n",
      "AVG REVIEW PER PERSON ::  2.2200117941568154\n"
     ]
    }
   ],
   "source": [
    "#TO EXTRACT VALUE FROM A CELL IN DATAFRAME\n",
    "print(\"TOTAL NUMBER OF USERS :: \", (unique.iloc[0,1]))\n",
    "print(\"TOTAL NUMBER OF REVIEWS :: \" , (unique.iloc[0,0]))\n",
    "print(\"AVG REVIEW PER PERSON :: \" , (unique.iloc[0,0]) / (unique.iloc[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b><center><font color = \"red\"> [1].</font> <u>Data Cleaning: Deduplication</u></center></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is observed (as shown in the table below) that the reviews data had many duplicate entries. Hence it was necessary to remove duplicates in order to get unbiased results for the analysis of the data. Following is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73791</td>\n",
       "      <td>B000HDOPZG</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>78445</td>\n",
       "      <td>B000HDL1RQ</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>138277</td>\n",
       "      <td>B000HDOPYM</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>138317</td>\n",
       "      <td>B000HDOPYC</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>155049</td>\n",
       "      <td>B000PAQ75C</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id   ProductId         UserId      ProfileName  HelpfulnessNumerator  \\\n",
       "0   73791  B000HDOPZG  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "1   78445  B000HDL1RQ  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "2  138277  B000HDOPYM  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "3  138317  B000HDOPYC  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "4  155049  B000PAQ75C  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "\n",
       "   HelpfulnessDenominator  Score        Time  \\\n",
       "0                       2      5  1199577600   \n",
       "1                       2      5  1199577600   \n",
       "2                       2      5  1199577600   \n",
       "3                       2      5  1199577600   \n",
       "4                       2      5  1199577600   \n",
       "\n",
       "                             Summary  \\\n",
       "0  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "1  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "2  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "3  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "4  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "\n",
       "                                                Text  \n",
       "0  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "1  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "2  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "3  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "4  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_sql_query(\"\"\"\n",
    "SELECT *\n",
    "FROM Reviews\n",
    "WHERE UserId = \"AR5J8UI46CURR\"\n",
    "\"\"\",con)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen that the USER ID, HELPFULNESS NUMERATOR , HELPFULNESS DENOMIATOR, SCORE, TIME, SUMMARY and TEXT are the same for all these reviews.\n",
    "\n",
    "As they all have the same timestamp, we can conclude that all these rows have been duplicated\n",
    "\n",
    "Now, after analysing the PRODUCT ID(s) we found that they all belong to the same brand having a same product but with different flavours. \n",
    "\n",
    "Hence in order to reduce redundancy it we should eliminate the rows having same parameters\n",
    "\n",
    "=============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>138706</th>\n",
       "      <td>150524</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>ACITT7DI6IDDL</td>\n",
       "      <td>shari zychinski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>939340800</td>\n",
       "      <td>EVERY book is educational</td>\n",
       "      <td>this witty little book makes my son laugh at l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138688</th>\n",
       "      <td>150506</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>A2IW4PEEKO2R0U</td>\n",
       "      <td>Tracy</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1194739200</td>\n",
       "      <td>Love the book, miss the hard cover version</td>\n",
       "      <td>I grew up reading these Sendak books, and watc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138689</th>\n",
       "      <td>150507</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>A1S4A3IQ2MU7V4</td>\n",
       "      <td>sally sue \"sally sue\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1191456000</td>\n",
       "      <td>chicken soup with rice months</td>\n",
       "      <td>This is a fun way for children to learn their ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138690</th>\n",
       "      <td>150508</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>AZGXZ2UUK6X</td>\n",
       "      <td>Catherine Hallberg \"(Kate)\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1076025600</td>\n",
       "      <td>a good swingy rhythm for reading aloud</td>\n",
       "      <td>This is a great little book to read aloud- it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138691</th>\n",
       "      <td>150509</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>A3CMRKGE0P909G</td>\n",
       "      <td>Teresa</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1018396800</td>\n",
       "      <td>A great way to learn the months</td>\n",
       "      <td>This is a book of poetry about the months of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176791</th>\n",
       "      <td>191721</td>\n",
       "      <td>B009UOFTUI</td>\n",
       "      <td>AJVB004EB0MVK</td>\n",
       "      <td>D. Christofferson</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1345852800</td>\n",
       "      <td>weak coffee not good for a premium product and...</td>\n",
       "      <td>This coffee supposedly is premium, it tastes w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1362</th>\n",
       "      <td>1478</td>\n",
       "      <td>B009UOFU20</td>\n",
       "      <td>AJVB004EB0MVK</td>\n",
       "      <td>D. Christofferson</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1345852800</td>\n",
       "      <td>weak coffee not good for a premium product and...</td>\n",
       "      <td>This coffee supposedly is premium, it tastes w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303285</th>\n",
       "      <td>328482</td>\n",
       "      <td>B009UUS05I</td>\n",
       "      <td>ARL20DSHGVM1Y</td>\n",
       "      <td>Jamie</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1331856000</td>\n",
       "      <td>Perfect</td>\n",
       "      <td>The basket was the perfect sympathy gift when ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5259</th>\n",
       "      <td>5703</td>\n",
       "      <td>B009WSNWC4</td>\n",
       "      <td>AMP7K1O84DH1T</td>\n",
       "      <td>ESTY</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1351209600</td>\n",
       "      <td>DELICIOUS</td>\n",
       "      <td>Purchased this product at a local store in NY ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302474</th>\n",
       "      <td>327601</td>\n",
       "      <td>B009WVB40S</td>\n",
       "      <td>A3ME78KVX31T21</td>\n",
       "      <td>K'la</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1351123200</td>\n",
       "      <td>Tasty!</td>\n",
       "      <td>I purchased this to send to my son who's away ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>525814 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId                  ProfileName  \\\n",
       "138706  150524  0006641040   ACITT7DI6IDDL              shari zychinski   \n",
       "138688  150506  0006641040  A2IW4PEEKO2R0U                        Tracy   \n",
       "138689  150507  0006641040  A1S4A3IQ2MU7V4        sally sue \"sally sue\"   \n",
       "138690  150508  0006641040     AZGXZ2UUK6X  Catherine Hallberg \"(Kate)\"   \n",
       "138691  150509  0006641040  A3CMRKGE0P909G                       Teresa   \n",
       "...        ...         ...             ...                          ...   \n",
       "176791  191721  B009UOFTUI   AJVB004EB0MVK            D. Christofferson   \n",
       "1362      1478  B009UOFU20   AJVB004EB0MVK            D. Christofferson   \n",
       "303285  328482  B009UUS05I   ARL20DSHGVM1Y                        Jamie   \n",
       "5259      5703  B009WSNWC4   AMP7K1O84DH1T                         ESTY   \n",
       "302474  327601  B009WVB40S  A3ME78KVX31T21                         K'la   \n",
       "\n",
       "        HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "138706                     0                       0      1   939340800   \n",
       "138688                     1                       1      1  1194739200   \n",
       "138689                     1                       1      1  1191456000   \n",
       "138690                     1                       1      1  1076025600   \n",
       "138691                     3                       4      1  1018396800   \n",
       "...                      ...                     ...    ...         ...   \n",
       "176791                     0                       0      0  1345852800   \n",
       "1362                       0                       0      0  1345852800   \n",
       "303285                     0                       0      1  1331856000   \n",
       "5259                       0                       0      1  1351209600   \n",
       "302474                     0                       0      1  1351123200   \n",
       "\n",
       "                                                  Summary  \\\n",
       "138706                          EVERY book is educational   \n",
       "138688         Love the book, miss the hard cover version   \n",
       "138689                      chicken soup with rice months   \n",
       "138690             a good swingy rhythm for reading aloud   \n",
       "138691                    A great way to learn the months   \n",
       "...                                                   ...   \n",
       "176791  weak coffee not good for a premium product and...   \n",
       "1362    weak coffee not good for a premium product and...   \n",
       "303285                                            Perfect   \n",
       "5259                                            DELICIOUS   \n",
       "302474                                             Tasty!   \n",
       "\n",
       "                                                     Text  \n",
       "138706  this witty little book makes my son laugh at l...  \n",
       "138688  I grew up reading these Sendak books, and watc...  \n",
       "138689  This is a fun way for children to learn their ...  \n",
       "138690  This is a great little book to read aloud- it ...  \n",
       "138691  This is a book of poetry about the months of t...  \n",
       "...                                                   ...  \n",
       "176791  This coffee supposedly is premium, it tastes w...  \n",
       "1362    This coffee supposedly is premium, it tastes w...  \n",
       "303285  The basket was the perfect sympathy gift when ...  \n",
       "5259    Purchased this product at a local store in NY ...  \n",
       "302474  I purchased this to send to my son who's away ...  \n",
       "\n",
       "[525814 rows x 10 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing redundancy on sorted data will be faster\n",
    "sorted_data = data.sort_values(\"ProductId\", axis = 0 , ascending = True , \\\n",
    "                              kind = 'quicksort' , na_position = 'last') #na_position is the position of NaN data\n",
    "sorted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(364173, 10)\n"
     ]
    }
   ],
   "source": [
    "cleaner_data = sorted_data.drop_duplicates(subset = {\"UserId\" , \"Time\" , \"ProfileName\" , \"Text\"} , keep = \"first\")\n",
    "print (cleaner_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another type of redundancy:-\n",
    "**Helpfulness Numerator > Helpfulness Denominator**\n",
    "\n",
    "Which is not possible. Hence, we drop such Reviews too\n",
    "\n",
    "Below is the example Reviews in which this happened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44737</td>\n",
       "      <td>B001EQ55RW</td>\n",
       "      <td>A2V0I904FH7ABY</td>\n",
       "      <td>Ram</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1212883200</td>\n",
       "      <td>Pure cocoa taste with crunchy almonds inside</td>\n",
       "      <td>It was almost a 'love at first bite' - the per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64422</td>\n",
       "      <td>B000MIDROQ</td>\n",
       "      <td>A161DK06JJMCYF</td>\n",
       "      <td>J. E. Stephens \"Jeanne\"</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1224892800</td>\n",
       "      <td>Bought This for My Son at College</td>\n",
       "      <td>My son loves spaghetti so I didn't hesitate or...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id   ProductId          UserId              ProfileName  \\\n",
       "0  44737  B001EQ55RW  A2V0I904FH7ABY                      Ram   \n",
       "1  64422  B000MIDROQ  A161DK06JJMCYF  J. E. Stephens \"Jeanne\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     3                       2      4  1212883200   \n",
       "1                     3                       1      5  1224892800   \n",
       "\n",
       "                                        Summary  \\\n",
       "0  Pure cocoa taste with crunchy almonds inside   \n",
       "1             Bought This for My Son at College   \n",
       "\n",
       "                                                Text  \n",
       "0  It was almost a 'love at first bite' - the per...  \n",
       "1  My son loves spaghetti so I didn't hesitate or...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_sql_query(\"\"\"\n",
    "SELECT *\n",
    "FROM Reviews\n",
    "WHERE HelpfulnessNumerator > HelpfulnessDenominator\"\"\",con)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only two such entries are found in the whole data set.\n",
    "\n",
    "Now we clean these out, if they exist, from our sample dataset (cleaner_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(364171, 10)\n"
     ]
    }
   ],
   "source": [
    "cleaner_data = cleaner_data[cleaner_data.HelpfulnessNumerator <= cleaner_data.HelpfulnessDenominator]\n",
    "print (cleaner_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of Data remained :: 69.26%\n"
     ]
    }
   ],
   "source": [
    "print(\"Percentage of Data remained :: {}%\".format( format(((cleaner_data.shape[0]*1.0) / (data.shape[0]*1.0))*100 , '0.2f')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(364171, 10)\n",
      "1    307061\n",
      "0     57110\n",
      "Name: Score, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print (cleaner_data.shape)\n",
    "print (cleaner_data.Score.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check for NaN entries ::**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id                        False\n",
      "ProductId                 False\n",
      "UserId                    False\n",
      "ProfileName               False\n",
      "HelpfulnessNumerator      False\n",
      "HelpfulnessDenominator    False\n",
      "Score                     False\n",
      "Time                      False\n",
      "Summary                   False\n",
      "Text                      False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "print (cleaner_data.isna().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no NaN values. Hence we move on to the next part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as we know that, that the way people write anything changes according to the time. For example, a product review might not get good reviews at the starting but as time progresses they make some changes and start getting more positive reviews.<br>\n",
    "So, we should sort the data by time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner_data = cleaner_data.sort_values(by=\"Time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b><center><font color = \"red\"> [2].</font> <u>Text Preprocessing</u></center></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b><font color = \"darkorange\"> [i].</font> Understanding Text Preprocessing</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now that we have finished deduplication our data requires some preprocessing before we go on further with analysis and making the prediction model.\n",
    "\n",
    "Hence in the Preprocessing phase we do the following in the order below:-\n",
    "\n",
    "1. Begin by removing the html tags\n",
    "2. Remove any punctuations or limited set of special characters like , or . or # etc.\n",
    "3. Check if the word is made up of english letters and is not alpha-numeric\n",
    "4. Check to see if the length of the word is greater than 2 (as it was researched that there is no adjective in 2-letters)\n",
    "5. Convert the word to lowercase\n",
    "6. Remove Stopwords\n",
    "7. Finally Snowball Stemming the word (it was obsereved to be better than Porter Stemming)<br>\n",
    "\n",
    "After which we collect the words used to describe positive and negative reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b><font color = \"green\"> (1). </font> REMOVING HTML TAGS</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some people, while writing reviews tend to insert links in it. And sometimes the text copied from a website might have html tags.<br>\n",
    "As they are of no use in sentiment analysis we should get rid of them<br>\n",
    "For this we can either use Regex module and subsititute accordingly or we can use BeautifulSoup from BS4 to remove these links and tags in just one line.\n",
    "\n",
    "Let us take an example :: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inititally :: \n",
      "\n",
      " For me, when the days get colder nothing is as rewarding as a simple cup of hot tea. And for it's claimed immunity benefits, a basic green tea is a common pick for maintaining a healthy natural balance during the flu season. From previous experiences in tasting the Tazo brand, both of the bottled and boxed products, they have proven to be unsurpassed for quality and flavor. Once I've tried their teas they immediately became my drink of choice. <p>The Zen Green Tea Blend is a wonderful one that has only a few ingredients with no artificial anything. And thankfully, doesn't boast the addition of fortified vitamins in some senseless amount. It truly is an enlightening blend of green tea, spearmint, lemongrass and lemon verbena. Thus making it versatile refreshment for anytime of the day, whether it's right after meals or between meals, or just before bedtime. Generally light and mild tasting, but that will depend upon how long you steep it and if you add a sweetener of some form.<p>Interesting too, are the amusing comments and remarks that appear on the packaging. Reading through this as you drink your tea makes it a distinctive experience. I never seen tea so clever! I wonder if consuming Tazo really does improve a person's outlook on life and affect his or her well being? I think it just could be the great taste and aroma and probably the reassuring thought of doing your health a favor.<br />Try a couple of their teas - they are sure to please!<br />And Tazo Zen Green Tea makes a good choice. \n",
      "\n",
      " ======================================================================\n",
      "After Removing Tags and links:: \n",
      "\n",
      "For me, when the days get colder nothing is as rewarding as a simple cup of hot tea. And for it's claimed immunity benefits, a basic green tea is a common pick for maintaining a healthy natural balance during the flu season. From previous experiences in tasting the Tazo brand, both of the bottled and boxed products, they have proven to be unsurpassed for quality and flavor. Once I've tried their teas they immediately became my drink of choice. The Zen Green Tea Blend is a wonderful one that has only a few ingredients with no artificial anything. And thankfully, doesn't boast the addition of fortified vitamins in some senseless amount. It truly is an enlightening blend of green tea, spearmint, lemongrass and lemon verbena. Thus making it versatile refreshment for anytime of the day, whether it's right after meals or between meals, or just before bedtime. Generally light and mild tasting, but that will depend upon how long you steep it and if you add a sweetener of some form.Interesting too, are the amusing comments and remarks that appear on the packaging. Reading through this as you drink your tea makes it a distinctive experience. I never seen tea so clever! I wonder if consuming Tazo really does improve a person's outlook on life and affect his or her well being? I think it just could be the great taste and aroma and probably the reassuring thought of doing your health a favor.Try a couple of their teas - they are sure to please!And Tazo Zen Green Tea makes a good choice. \n",
      " ======================================================================\n"
     ]
    }
   ],
   "source": [
    "x = cleaner_data.Text.values[57]\n",
    "print (\"Inititally :: \\n\\n\" , x , \"\\n\\n\" , '='*70)\n",
    "print (\"After Removing Tags and links:: \\n\" )\n",
    "x =  re.sub(r\"http\\S+\" , \"\" , x)\n",
    "x = BeautifulSoup(x , 'lxml').get_text()\n",
    "print (x , \"\\n\" , \"=\"*70)\n",
    "# r is used for creating regular expressions, \n",
    "# \\S used for non-whitespace\n",
    "# + used for sequence i.e. \\S+   ----> sequence of non-whitespace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see all *TAGS* and *LINKS* have been removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b><font color = \"green\"> (2). </font> PERFORMING DECONTRACTION (FUNCTION)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to replace words that use short forms such as --> I'm or won't or couldn't or wouldn't etc. to their full form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontract (phrase):\n",
    "    import contractions\n",
    "    return contractions.fix(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data ::\n",
      "\n",
      "\n",
      "For me, when the days get colder nothing is as rewarding as a simple cup of hot tea. And for it's claimed immunity benefits, a basic green tea is a common pick for maintaining a healthy natural balance during the flu season. From previous experiences in tasting the Tazo brand, both of the bottled and boxed products, they have proven to be unsurpassed for quality and flavor. Once I've tried their teas they immediately became my drink of choice. The Zen Green Tea Blend is a wonderful one that has only a few ingredients with no artificial anything. And thankfully, doesn't boast the addition of fortified vitamins in some senseless amount. It truly is an enlightening blend of green tea, spearmint, lemongrass and lemon verbena. Thus making it versatile refreshment for anytime of the day, whether it's right after meals or between meals, or just before bedtime. Generally light and mild tasting, but that will depend upon how long you steep it and if you add a sweetener of some form.Interesting too, are the amusing comments and remarks that appear on the packaging. Reading through this as you drink your tea makes it a distinctive experience. I never seen tea so clever! I wonder if consuming Tazo really does improve a person's outlook on life and affect his or her well being? I think it just could be the great taste and aroma and probably the reassuring thought of doing your health a favor.Try a couple of their teas - they are sure to please!And Tazo Zen Green Tea makes a good choice. \n",
      "\n",
      "======================================================================\n",
      "Decontracted Data ::\n",
      "\n",
      "\n",
      "For me, when the days get colder nothing is as rewarding as a simple cup of hot tea. And for it is claimed immunity benefits, a basic green tea is a common pick for maintaining a healthy natural balance during the flu season. From previous experiences in tasting the Tazo brand, both of the bottled and boxed products, they have proven to be unsurpassed for quality and flavor. Once I have tried their teas they immediately became my drink of choice. The Zen Green Tea Blend is a wonderful one that has only a few ingredients with no artificial anything. And thankfully, does not boast the addition of fortified vitamins in some senseless amount. It truly is an enlightening blend of green tea, spearmint, lemongrass and lemon verbena. Thus making it versatile refreshment for anytime of the day, whether it is right after meals or between meals, or just before bedtime. Generally light and mild tasting, but that will depend upon how long you steep it and if you add a sweetener of some form.Interesting too, are the amusing comments and remarks that appear on the packaging. Reading through this as you drink your tea makes it a distinctive experience. I never seen tea so clever! I wonder if consuming Tazo really does improve a person's outlook on life and affect his or her well being? I think it just could be the great taste and aroma and probably the reassuring thought of doing your health a favor.Try a couple of their teas - they are sure to please!And Tazo Zen Green Tea makes a good choice.\n"
     ]
    }
   ],
   "source": [
    "print (\"Original Data ::\\n\\n\")\n",
    "print (x , \"\\n\")\n",
    "print (\"=\"*70)\n",
    "print (\"Decontracted Data ::\\n\\n\")\n",
    "print (decontract(x) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b> <font color = \"green\"> (3). </font> REMOVING ALL  SPECIAL CHARACTERS AND PUNCTUATION</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a regex to match a string of characters that are not letters or numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For me when the days get colder nothing is as rewarding as a simple cup of hot tea And for it s claimed immunity benefits a basic green tea is a common pick for maintaining a healthy natural balance during the flu season From previous experiences in tasting the Tazo brand both of the bottled and boxed products they have proven to be unsurpassed for quality and flavor Once I ve tried their teas they immediately became my drink of choice The Zen Green Tea Blend is a wonderful one that has only a few ingredients with no artificial anything And thankfully doesn t boast the addition of fortified vitamins in some senseless amount It truly is an enlightening blend of green tea spearmint lemongrass and lemon verbena Thus making it versatile refreshment for anytime of the day whether it s right after meals or between meals or just before bedtime Generally light and mild tasting but that will depend upon how long you steep it and if you add a sweetener of some form Interesting too are the amusing comments and remarks that appear on the packaging Reading through this as you drink your tea makes it a distinctive experience I never seen tea so clever I wonder if consuming Tazo really does improve a person s outlook on life and affect his or her well being I think it just could be the great taste and aroma and probably the reassuring thought of doing your health a favor Try a couple of their teas they are sure to please And Tazo Zen Green Tea makes a good choice \n"
     ]
    }
   ],
   "source": [
    "x = re.sub('[^A-Za-z0-9]+' , \" \" , x)\n",
    "print (x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b><font color = \"green\"> (4). </font> REMOVING ALPHANUMERIC LETTERS</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words that contain alphabets as well as numbers should be removed. e.g - 'B00004RBDY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blah blah blah  blah blah blahhh\n"
     ]
    }
   ],
   "source": [
    "x = \"blah blah blah B004RBDY blah blah blahhh\"\n",
    "print ( re.sub(\"\\S*\\d\\S*\" , \"\" , x ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*As you can see alphanumeric words have been removed*\n",
    "\n",
    "1. \\S*  --> Means all character excluding white spaces might occurs 0 or more times\n",
    "2. \\d  ----> digit must occur one time.\n",
    "3. \\S*\\d\\S*  ---> eg:- saj3434jkdsd\n",
    "4. This will remove all digits and string with digits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b><font color = \"green\"> (5). </font> CONVERTING INTO LOWER CASE</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case(phrase):\n",
    "    return ' '.join(word.lower() for word in phrase.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example :: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initially :: \n",
      " Michael Keaton brings no distinguishing characteristics to the ghoul 'Beetlejuice', he merely acts bizarre, as does the script. It is often stunning cinematography but when the movie itself comes into focus, it's like finding one of Beetlejuice's snacks in your popcorn.\n",
      "\n",
      "After LowerCase :: \n",
      " michael keaton brings no distinguishing characteristics to the ghoul 'beetlejuice', he merely acts bizarre, as does the script. it is often stunning cinematography but when the movie itself comes into focus, it's like finding one of beetlejuice's snacks in your popcorn.\n"
     ]
    }
   ],
   "source": [
    "print (\"Initially :: \\n\" , cleaner_data.Text.values[22])\n",
    "print (\"\\nAfter LowerCase :: \\n\" , lower_case(cleaner_data.Text.values[22]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b><font color = \"green\"> (6). </font> REMOVING STOP WORDS</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the set of stop words we are\n",
    "1. Removing - \"no\" , \"nor\" , \"not\"\n",
    "2. Adding - br (as in previous steps it can be seen that <br /> becomes br)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177 179\n"
     ]
    }
   ],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "stop.add(\"br\")\n",
    "stop.remove(\"no\")\n",
    "stop.remove(\"nor\")\n",
    "stop.remove(\"not\")\n",
    "print (len(stop) , len(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mustn', 'o', \"isn't\", 'own', 'each', 'as', 'ma', 'your', \"aren't\", \"she's\", 'mightn', 'once', 'between', 'and', \"you'll\", 'up', 'ours', 'when', 'our', 'below', 'what', 'during', 'most', 'further', \"won't\", 'than', 'about', 'who', 'such', 'before', \"shan't\", 'those', 'very', 'now', \"hadn't\", 'an', 'the', 'only', 'my', 'doing', 'having', 'd', 'which', 'himself', 'or', 'off', 'do', 'y', 'does', 'have', 'won', \"don't\", 'been', 'was', 'hadn', \"couldn't\", 'at', 'these', 'shan', 'you', 'its', 'too', \"doesn't\", \"mustn't\", 'couldn', 'herself', 'out', 'then', 'can', 'had', 'yourself', 'just', 'they', 'br', 'their', \"mightn't\", \"hasn't\", 'shouldn', 'both', 'don', \"you're\", 'will', 't', 'has', 'whom', 'above', 'her', 'doesn', 'under', 'how', 'some', 'them', 'there', 'because', 'should', 'while', 'theirs', 'with', 'weren', 'for', 'is', 'if', 'that', 'i', 'hasn', \"haven't\", 'he', 'isn', 'to', \"that'll\", 'by', 'against', 'other', \"wasn't\", 'so', 'needn', 'themselves', 'wouldn', 've', 'down', 'are', 'yourselves', 'm', 'all', \"didn't\", 'll', 'through', 'him', 'more', 'here', 'myself', \"it's\", 'didn', 'a', 'she', 'were', 'few', 'wasn', 'hers', 'this', \"shouldn't\", 'of', 're', \"you'd\", 'we', 'over', 'why', 'after', 'am', 'same', 'being', 'but', 'itself', \"you've\", 's', \"should've\", 'where', \"weren't\", 'ain', 'it', 'be', 'again', 'his', 'until', 'ourselves', 'did', 'from', 'haven', 'any', 'yours', 'into', \"needn't\", 'aren', 'me', 'on', 'in', \"wouldn't\"}\n"
     ]
    }
   ],
   "source": [
    "print (stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to remove stopwords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem_stopwords(phrase):\n",
    "    return ' '.join(word for word in phrase.split() if word not in stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am continually amazed at the shoddy treatment that some movies get in their DVD release.  This DVD is simply a disgrace, especially considering what a great movie this is.  I give the movie itself 5 stars; it's a wonderful example of Tim Burton's energy and style.<p>This DVD has no extras worth mentioning.  No deleted scenes, no featurettes, not even a lousy commentary track!  To make it even worse, the film has been CUT DOWN from the theatrical release!  I have never seen a DVD release before where you get LESS than was originally presented in theaters.<p>My advice is to save your money until somebody figures out that when a movie is released on DVD, it needs to live up to the capabilities of the medium, and should always provide more material than was originally released, not less.\n",
      "\n",
      " ====================================================================== \n",
      "\n",
      "I continually amazed shoddy treatment movies get DVD release This DVD simply disgrace especially considering great movie I give movie 5 stars wonderful example Tim Burton energy style p This DVD no extras worth mentioning No deleted scenes no featurettes not even lousy commentary track To make even worse film CUT DOWN theatrical release I never seen DVD release get LESS originally presented theaters p My advice save money somebody figures movie released DVD needs live capabilities medium always provide material originally released not less\n"
     ]
    }
   ],
   "source": [
    "print (cleaner_data.Text.values[23])\n",
    "print (\"\\n\" , \"=\"*70 , \"\\n\")\n",
    "print (rem_stopwords(re.sub(\"[^A-Za-z0-9]+\" , ' ' , cleaner_data.Text.values[23])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b><font color = \"green\"> (7). </font> STEMMING WORDS</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use SnowballStemmer as it is better than PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man\n"
     ]
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "print (stemmer.stem(\"manly\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b><font color = \"darkorange\"> [ii].</font> Applying Text Preprocessing(All Together)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "364171it [05:39, 1072.03it/s]\n"
     ]
    }
   ],
   "source": [
    "null_review =[]\n",
    "stemmed_preprocessed_reviews = []\n",
    "unstemmed_preprocessed_reviews = [] #As word2vec works better with unstemmed text\n",
    "\n",
    "for count , review in tqdm(enumerate(cleaner_data.Text.values)):\n",
    "    #(1). REMOVING HTML TAGS :: \n",
    "    review  = re.sub(r\"http\\S+\" , '' , review)\n",
    "    #(2). REMOVING ALL OTHER TAGS\n",
    "    review = BeautifulSoup(review).get_text()\n",
    "    \n",
    "#     There were many cases where review became ' '(empty), after implementing this line of code \"re.sub(r'http\\S+','',review)\" \\n\n",
    "#     For example review number 24:-\n",
    "#     '<a href=\"http://www.amazon.com/gp/product/B0000VMBDI\">WILTON 13 PC GOLF SET 1306-7274</a><br /><br />I am very happ\n",
    "#     Here, if http(link) is removed, \"http://www.amazon.com/gp/product/B0000VMBDI\">WILTON\" This string will be removed.\n",
    "#     Hence removing \">\", which will create incorrect index.\n",
    "#     Thus BeautifulSoup(review).get_text() will return ' '\n",
    "#     BeautifulSoup(review).get_text() will result in empty string. For example check review number 24,121,456 and 601\n",
    "#     That's why I have used the code below\n",
    "\n",
    "    if review == '' or review == ' ':\n",
    "        null_review.append(count)\n",
    "        continue\n",
    "    \n",
    "    #(3). DECONTRACTION\n",
    "    review = decontract(review)\n",
    "    \n",
    "    #(4). REMOVING SPECIAL CHARACTERS AND PUNCTUATION\n",
    "    review = re.sub(\"[^A-Za-z0-9]+\" , ' ' , review)\n",
    "    \n",
    "    #(5). REMOVING ALPHANUMERIC WORDS\n",
    "    review = re.sub('\\S*\\d\\S*' , '' , review)\n",
    "    \n",
    "    #(6). LOWER CASING AND REMOVING STOP WORDS \n",
    "    review = lower_case(rem_stopwords(review))\n",
    "    \n",
    "    unstemmed_preprocessed_reviews.append(review)\n",
    "    \n",
    "    #(7). STEMMING\n",
    "    stemmed_preprocessed_reviews.append(' '.join(list(map(stemmer.stem , review.split()))))\n",
    "    #print (preprocessed_reviews)\n",
    "    #i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL STEMMED PROCESSED REVIEWS ::  363223\n",
      "TOTAL UNSTEMMED PROCESSED REVIEWS ::  363223\n",
      "TOTAL EMPTIED REVIEWS ::  948\n",
      "SOME OF THE EMPTIED REVIEWS :: \n",
      "\t [10283, 10328, 10846, 10859, 11280, 11379, 11623, 11647, 11891, 11943]\n"
     ]
    }
   ],
   "source": [
    "print(\"TOTAL STEMMED PROCESSED REVIEWS :: \" , len(stemmed_preprocessed_reviews))\n",
    "print(\"TOTAL UNSTEMMED PROCESSED REVIEWS :: \" , len(unstemmed_preprocessed_reviews))\n",
    "print(\"TOTAL EMPTIED REVIEWS :: \" , len(null_review))\n",
    "print(\"SOME OF THE EMPTIED REVIEWS :: \\n\\t\" , null_review[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example:\n",
      "Stemmed Review -  i rememb see show air televis year ago i child my sister later bought lp i day i thirti someth i use seri book song i student teach preschool turn whole school i purchas cd along book children the tradit live\n",
      "\n",
      "Unstemmed Review -  i remember seeing show aired television years ago i child my sister later bought lp i day i thirty something i used series books songs i student teaching preschoolers turned whole school i purchasing cd along books children the tradition lives\n"
     ]
    }
   ],
   "source": [
    "print (\"Example:\\nStemmed Review - \" , stemmed_preprocessed_reviews[1])\n",
    "print (\"\\nUnstemmed Review - \" , unstemmed_preprocessed_reviews[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b><font color = \"darkblue\"> Saving the Preprocessed Reviews in a New Database</font></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = list(range(cleaner_data.shape[0]))\n",
    "cleaner_data[\"index\"] = ind\n",
    "cleaner_data.set_index(\"index\" , inplace = True)\n",
    "final = cleaner_data.drop(null_review)\n",
    "final[\"index\"] = list(range(final.shape[0]))\n",
    "final.set_index('index' , inplace = True)\n",
    "\n",
    "final[\"CleanedText\"] = stemmed_preprocessed_reviews\n",
    "final[\"Unstemmed_CleanedText\"] = unstemmed_preprocessed_reviews\n",
    "#final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('final.sqlite')\n",
    "c = conn.cursor()\n",
    "conn.text_factory = str\n",
    "final.to_sql('Reviews' , conn  , schema = None , if_exists = 'replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b><font color = 'darkblue'> Loading the Preprocessed Reviews from the Saved Database</font></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> 363223\n",
      "<class 'list'> 363223\n"
     ]
    }
   ],
   "source": [
    "# os.getcwd()\n",
    "conn = sqlite3.connect('final.sqlite')\n",
    "final = pd.read_sql_query(\"\"\"\n",
    "SELECT *\n",
    "FROM Reviews\"\"\",conn)\n",
    "stemmed_preprocessed_reviews = final.CleanedText.values\n",
    "unstemed_preprocessed_reviews = final.Unstemmed_CleanedText.values\n",
    "print (type(stemmed_preprocessed_reviews) , len(stemmed_preprocessed_reviews))\n",
    "print (type(unstemmed_preprocessed_reviews) , len(unstemmed_preprocessed_reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center><font color = \"red\"> [3]. </font> <u>Using Different ways to Featurize Text</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to somehow convert all the text into vectors(numbers) as its easy for computer to work with numbers than text.<br>\n",
    "There are many ways availbale to build features that represent text data as numbers. For example:\n",
    "\n",
    "1.<b> Bag of Words</b>\n",
    "- Simple Bag of Words (uni-bi-n-gram)\n",
    "- TF-IDF Weighted Bag of Words (uni-bi-n-gram)<br>\n",
    "\n",
    "2.<b> Word2Vec</b>\n",
    "- Simple Word2Vec\n",
    "- TF-IDF Weighted Word2Vec (uni-bi-n-gram)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take only a few reviews to understand text featurizaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_sample = stemmed_preprocessed_reviews[:30000]\n",
    "unstemmed_sample = unstemmed_preprocessed_reviews[:30000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b><font color = \"darkorange\"> [i].</font> BAG OF WORDS!</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <b><font color = \"darkgreen\">(1).</font> Simple Bag of Words</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some Features :: \n",
      " ['alert' 'alfalfa' 'alfredo' 'alik' 'alittl' 'aliv' 'alkali' 'all']\n",
      "======================================================================\n",
      "TYPE OF BOW ::  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "SHAPE OF SPARSE MATRIX ::  (30000, 5000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "cv = CountVectorizer(min_df = 10 , max_features = 5000)\n",
    "\n",
    "bow = cv.fit_transform(stemmed_sample)\n",
    "bow = Normalizer().fit_transform(bow)\n",
    "\n",
    "print (\"Some Features :: \\n\" , cv.get_feature_names_out()[100:108])\n",
    "print (\"=\"*70)\n",
    "print (\"TYPE OF BOW :: \" , type(bow))\n",
    "print (\"SHAPE OF SPARSE MATRIX :: \" , bow.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b><font color = \"darkgreen\">(2).</font>TF-IDF Weighted Bag Of Words</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TYPE OF TFIDF BOW ::  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "SHAPE OF SPARSE MATRIX ::  (30000, 5000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(min_df = 10 , max_features = 5000)\n",
    "\n",
    "tfidf = tfidf_vect.fit_transform(stemmed_sample)\n",
    "tfidf = Normalizer().fit_transform(tfidf)\n",
    "\n",
    "print (\"TYPE OF TFIDF BOW :: \" , type(tfidf))\n",
    "print (\"SHAPE OF SPARSE MATRIX :: \" , tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b><font color = \"blue\"> Function to get top N features</font></b>\n",
    "\n",
    "Fuction to get the top 'N' TF-IDF features of a row(doc/review) and return them corresponding to the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_tfidf_feats (row , features , top_n):\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    \n",
    "    # [::-1] --> reversing the sorted array into descending form\n",
    "    # [:top_n] --> selecting the top n indices\n",
    "    \n",
    "    top_feats = [(features[i] , row[i]) for i in topn_ids]\n",
    "    \n",
    "    # Create a list of tupples using the indices in the array topn_ids\n",
    "    \n",
    "    df = pd.DataFrame(top_feats)\n",
    "    \n",
    "    # Convert into dataframe\n",
    "    \n",
    "    df.columns = ['feature' , 'tfidf']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aarat\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>book</td>\n",
       "      <td>0.504049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>son</td>\n",
       "      <td>0.278646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>loud</td>\n",
       "      <td>0.234095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>silli</td>\n",
       "      <td>0.218336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sing</td>\n",
       "      <td>0.216021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  feature     tfidf\n",
       "0    book  0.504049\n",
       "1     son  0.278646\n",
       "2    loud  0.234095\n",
       "3   silli  0.218336\n",
       "4    sing  0.216021"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_tf_idf = top_tfidf_feats (tfidf[0,:].toarray()[0] , tfidf_vect.get_feature_names() , 5)\n",
    "\n",
    "#.toarray()[0] --> To create a perfect one dimensional array which has no rows only columns :: vector.\n",
    "\n",
    "top_tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b><font color = \"darkgreen\">(3).</font>Simple Bag of Words and TF-IDF weighted Bag of Words using Bi-Grams\\N-Grams</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some Features of BOW bi-gram :: \n",
      " ['also add' 'also avail' 'also contain' 'also enjoy' 'also good'\n",
      " 'also great' 'also help' 'also like']\n",
      "Some Features of TFIDF weighted BOW bi-gram :: \n",
      " ['also add' 'also avail' 'also contain' 'also enjoy' 'also good'\n",
      " 'also great' 'also help' 'also like']\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "cv = CountVectorizer( ngram_range = (1,2) , max_features = 5000)\n",
    "tfidf_vect = TfidfVectorizer(ngram_range = (1,2) , max_features = 5000)\n",
    "\n",
    "bow = cv.fit_transform(stemmed_sample)\n",
    "tfidf = tfidf_vect.fit_transform(stemmed_sample)\n",
    "\n",
    "bow = Normalizer().fit_transform(bow)\n",
    "tfidf = Normalizer().fit_transform(tfidf)\n",
    "\n",
    "print (\"Some Features of BOW bi-gram :: \\n\" , cv.get_feature_names_out()[100:108])\n",
    "print (\"Some Features of TFIDF weighted BOW bi-gram :: \\n\" , tfidf_vect.get_feature_names_out()[100:108])\n",
    "print (\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b><font color = \"blue\"> Comparing Uni-Gram and Bi-Gram</font></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features using Uni-Gram only -  23249\n",
      "Number of features using Bi-Gram only -  487314\n",
      "Number of features using Uni+bi-Gram - 510563\n"
     ]
    }
   ],
   "source": [
    "cv11 = CountVectorizer(ngram_range = (1,1))\n",
    "cv22 = CountVectorizer(ngram_range = (2,2))\n",
    "cv12 = CountVectorizer(ngram_range = (1,2))\n",
    "\n",
    "print(\"Number of features using Uni-Gram only - \" , cv11.fit_transform(stemmed_sample).shape[1])\n",
    "print(\"Number of features using Bi-Gram only - \" , cv22.fit_transform(stemmed_sample).shape[1])\n",
    "print(\"Number of features using Uni+bi-Gram -\" , cv12.fit_transform(stemmed_sample).shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the bigram increases the features drastically. Therefore we must, even in the case of Uni-Grams, use the attribute max_features accordingly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b><font color = \"darkorange\"> [ii].</font>WORD2VEC!</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes semantic meaning of the words into account where as all the above techniques did not. It learns relationship between different words automatically from raw text.\n",
    "\n",
    "<b><font color = \"red\">It should be kept in mind that pretrained w2v models do not support stemmed words</font></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <b><font color = \"darkgray\"> To Download and Load The Pretrained Models</font> </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### <u> <font color = \"orange\"> To Download and Load The Pretrained Modelsfrom gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors \n",
    "import pickle\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fasttext-wiki-news-subwords-300',\n",
       " 'conceptnet-numberbatch-17-06-300',\n",
       " 'word2vec-ruscorpora-300',\n",
       " 'word2vec-google-news-300',\n",
       " 'glove-wiki-gigaword-50',\n",
       " 'glove-wiki-gigaword-100',\n",
       " 'glove-wiki-gigaword-200',\n",
       " 'glove-wiki-gigaword-300',\n",
       " 'glove-twitter-25',\n",
       " 'glove-twitter-50',\n",
       " 'glove-twitter-100',\n",
       " 'glove-twitter-200',\n",
       " '__testing_word2vec-matrix-synopsis']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "list(gensim.downloader.info()['models'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove_vectors = gensim.downloader.load('glove-twitter-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_file = datapath('C:/Users/aarat/gensim-data/glove-twitter-100/glove-twitter-100.txt')\n",
    "model_twt_100 = KeyedVectors.load_word2vec_format(glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOST SIMILAR WORDS TO MAN :: \n",
      " [('boy', 0.7652448415756226), ('dude', 0.7523702383041382)]\n",
      "\n",
      "SIMILARITY BETWEEN MAN AND WOMAN ::  0.6703952\n"
     ]
    }
   ],
   "source": [
    "print(\"MOST SIMILAR WORDS TO MAN :: \\n\" , model_twt_100.most_similar(\"man\")[0:2])\n",
    "print(\"\\nSIMILARITY BETWEEN MAN AND WOMAN :: \" , model_twt_100.similarity(\"man\" , \"woman\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_twt_100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <b><font color = \"darkblue\"> Train Your Own w2v Model</font></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 363223/363223 [00:03<00:00, 108232.78it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for review in tqdm(unstemmed_preprocessed_reviews):\n",
    "    corpus.append(review.split())\n",
    "\n",
    "\n",
    "w2v_model = Word2Vec(corpus , min_count = 5 , vector_size = 150 , workers = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b><u> <font color = \"darkorange\">Save The Model</font></u></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.save_word2vec_format('w2v_model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b><u> <font color = \"darkorange\">Load The Model</font></u></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = KeyedVectors.load_word2vec_format(\"w2v_model.bin\" , binary = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b><u> <font color = \"darkorange\"> Performance of trained w2v Model</font></u></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = w2v_model.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('great', 0.7840190529823303), ('decent', 0.747307300567627), ('awesome', 0.6455490589141846), ('bad', 0.6423206329345703), ('fantastic', 0.6403504610061646)]\n",
      "\n",
      "\n",
      "Similarity between 'good' and 'tasty' =  0.5088861\n",
      "\n",
      "\n",
      "[('scent', 0.7904716730117798), ('smells', 0.7542111873626709), ('odor', 0.7500920295715332), ('smelled', 0.7453361749649048), ('smelling', 0.736371636390686)]\n"
     ]
    }
   ],
   "source": [
    "print(w2v_model.most_similar('good')[:5])\n",
    "print('\\n')\n",
    "print(\"Similarity between 'good' and 'tasty' = \" , w2v_model.similarity('good','tasty'))\n",
    "print('\\n')\n",
    "print(w2v_model.most_similar('smell')[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33588\n",
      "<class 'dict'>\n",
      "Some words : ['everyday', 'standard', 'shampoo', 'known', 'peach', 'various', 'brewing', 'was', 'expiration', 'jelly']\n"
     ]
    }
   ],
   "source": [
    "print(len(w2v_model.key_to_index)) # Total number of words in our vocabulary\n",
    "print(type(w2v_model.key_to_index))\n",
    "print(\"Some words :\",list(w2v_model.key_to_index)[1000:1010])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <b><font color = \"darkgreen\">(1).</font> Converting All Reviews To Vectors Using Avg-W2V</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 363223/363223 [01:06<00:00, 5500.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSIONS OF REVIEW VECOTRS :: (363223, 150)\n"
     ]
    }
   ],
   "source": [
    "avg_w2v_review_vectors = []\n",
    "\n",
    "for review in tqdm(unstemmed_preprocessed_reviews):# preprocessed_reviews = 36k\n",
    "    vec = np.zeros(150) # as we created 150 dimensional w2v\n",
    "    count = 0\n",
    "    for word in review.split():\n",
    "        if word in vocabulary:\n",
    "            vec += w2v_model[word]\n",
    "            count += 1\n",
    "    if count > 0:\n",
    "        vec /= count\n",
    "    \n",
    "    avg_w2v_review_vectors.append(vec)\n",
    "    \n",
    "avg_w2v_review_vectors = np.array(avg_w2v_review_vectors)\n",
    "avg_w2v_review_vectors = Normalizer().fit_transform(avg_w2v_review_vectors)\n",
    "\n",
    "print(\"DIMENSIONS OF REVIEW VECOTRS ::\" , avg_w2v_review_vectors.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <b><font color = \"darkgreen\">(2).</font> Converting All Reviews To Vectors Using TF-IDF Weighted Avg-W2V</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aarat\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "tfidf_vect = TfidfVectorizer(max_features = 5000)\n",
    "\n",
    "tfidf = tfidf_vect.fit_transform(unstemmed_preprocessed_reviews)\n",
    "# tfidf = Normalizer().fit_transform(tfidf)\n",
    "tfidf_feats = tfidf_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 363223/363223 [31:57<00:00, 189.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSIONS OF REVIEW VECOTRS :: (363223, 150)\n"
     ]
    }
   ],
   "source": [
    "tfidf_w2v_review_vectors = []\n",
    "\n",
    "row = 0\n",
    "for review in tqdm(unstemmed_preprocessed_reviews):\n",
    "    review_vec = np.zeros(150)\n",
    "    weighted_sum = 0\n",
    "    \n",
    "    for word in review.split():\n",
    "        if word in vocabulary and word in tfidf_feats:\n",
    "            \n",
    "            vec = w2v_model[word]\n",
    "            tfidf_val = tfidf[row , tfidf_feats.index(word)]\n",
    "            \n",
    "            review_vec += (vec * tfidf_val)\n",
    "            weighted_sum += tfidf_val\n",
    "            \n",
    "    if weighted_sum > 0:\n",
    "        review_vec /= weighted_sum\n",
    "        \n",
    "    tfidf_w2v_review_vectors.append(review_vec)\n",
    "    row += 1\n",
    "\n",
    "tfidf_w2v_review_vectors = np.array(tfidf_w2v_review_vectors)\n",
    "tfidf_w2v_review_vectors = Normalizer().fit_transform(tfidf_w2v_review_vectors)\n",
    "\n",
    "print(\"DIMENSIONS OF REVIEW VECOTRS ::\" , tfidf_w2v_review_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b><center>End<center></b></h3>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
